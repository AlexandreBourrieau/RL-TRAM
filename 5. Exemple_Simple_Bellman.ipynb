{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exemple_Simple_Bellman.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Approche avec les équations de Bellman**"
      ],
      "metadata": {
        "id": "XGJ8yJ8W_3sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7. Exemple**"
      ],
      "metadata": {
        "id": "sZ1BfXx-VlXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons maintenant un exemple simple afin de synthétiser l'ensemble des concepts que nous avons vu : valeur d'action, valeur d'état et équation de Bellman."
      ],
      "metadata": {
        "id": "kVI_HWPH-hyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**7.1 Problématique**"
      ],
      "metadata": {
        "id": "35ZcLNn--md3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On considère un environnement composé de 3 états A, B et C. L'agent commence sur l'état A. Le but de l'agent est d'atteindre la cible C. L'état B est un piège. L'agent peut choisir parmi 4 actions mais pour simplifier les notations on les considère comme ceci :\n",
        "- 0 : Gauche ou Droite\n",
        "- 1 : Haut ou bas"
      ],
      "metadata": {
        "id": "nv6DBoNlXNnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL16.png?raw=true\" width=\"240\"></img></center>"
      ],
      "metadata": {
        "id": "kOxQIjjiZWwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **fonction valeur d'action** dans ce cas peut être exprimée sous forme d'un tableau qui fait correspondre les valeurs de chaque paire état-action. Au début, celle-ci est initialisée à 0 :"
      ],
      "metadata": {
        "id": "zbR9LgRCZgtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<thead><tr>\n",
        "<th style=\"text-align:center\">État: S</th>\n",
        "<th style=\"text-align:center\">Action: A</th>\n",
        "<th style=\"text-align:center\">Valeur</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">B</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">B</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">C</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">C</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Ip3mEFfWaTEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour mettre en place l'apprentissage, il faut également définir une table des transitions ainsi que les récompenses. Prenons par exemple la table des transitions suivantes (pour simplifier, on considère que les probabilités de transition et les récompenses sont identiques pour chaque état). La table ci-dessous ne montre que ce qui concerne l'état A et B mais on suivra la même idée pour l'état C:"
      ],
      "metadata": {
        "id": "t7FhoxKnazU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<thead><tr>\n",
        "<th style=\"text-align:center\">État: S</th>\n",
        "<th style=\"text-align:center\">Action: A</th>\n",
        "<th style=\"text-align:center\">État suivant: S'</th>\n",
        "<th style=\"text-align:center\">Probabilité de transition: P</th>\n",
        "<th style=\"text-align:center\">Récompense: R</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0.2</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">B</td>\n",
        "<td style=\"text-align:center\">0.5</td>\n",
        "<td style=\"text-align:center\">-1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "<td style=\"text-align:center\">C</td>\n",
        "<td style=\"text-align:center\">0.3</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">0.3</td>\n",
        "<td style=\"text-align:center\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">B</td>\n",
        "<td style=\"text-align:center\">0.2</td>\n",
        "<td style=\"text-align:center\">-1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">A</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "<td style=\"text-align:center\">C</td>\n",
        "<td style=\"text-align:center\">0.5</td>\n",
        "<td style=\"text-align:center\">1</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "KvcBdTylbTzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**7.2 Programme Python**"
      ],
      "metadata": {
        "id": "jeRJBVZ--wfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Définition de la table des transitions**"
      ],
      "metadata": {
        "id": "J6c8uQcEBgWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction suivante permet de construire la table des transitions. Pour chaque état de l'environnement et pour chaque action pouvant être effectuée sur cet état, la table contient les informations suivantes:\n",
        "\n",
        "- L'index de l'états suivant sur lequel arrive l'agent après avoir sélectionné une action donnée sur un état donné \n",
        "- La probabilité de transition d'arriver sur cet état suivant\n",
        "- La récompense obtenue suite à cette transition"
      ],
      "metadata": {
        "id": "Rj8wB-1cAZ-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons donc besoin de définir un tenseur qui pourra contenir les trois informations précédentes (index de l'état suivant, probabilité de transition et récompense) pour chaque paire (état, action).\n",
        "\n",
        "L'idée est donc de fusionner trois tenseurs:\n",
        "- Tenseur n°1 pour accéder aux états suivants : (nbr_etat,nbr_actions,nbr_etats_suivants)\n",
        "- Tenseur n°2 pour accéder aux probabilités : (nbr_etat,nbr_actions,nbr_etats_suivants)\n",
        "- Tenseur n°3 pour accéder aux récompenses : (nbr_etat,nbr_actions,nbr_etats_suivants)"
      ],
      "metadata": {
        "id": "lYbRofBpQJqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour faire cela, on peut par exemple ajouter une dimension sur l'axe 0 et définir un tenseur au format :\n",
        "\n",
        "$\\quad$(3,nbr_etat,nbr_actions,nbr_etats_suivants)\n",
        "\n",
        "Sachant que:\n",
        "- l'indice 0 de l'axe 0 ciblera les états suivants\n",
        "- l'indice 1 de l'axe 0 ciblera les probabilités\n",
        "- l'indice 2 de l'axe 0 ciblera les récompenses\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LTZ19byoRfv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, nous allons plutôt construire un tenseur numpy avec un type personnalisé. Cela rendra la manipulation des données plus intuitive."
      ],
      "metadata": {
        "id": "4KeOyK-Payc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "nombre_etats = 3\n",
        "nombre_actions = 2\n",
        "\n",
        "#####################################################\n",
        "# Construction de la table des transitions\n",
        "# Format :  (nbr_etats, nbr_actions, nbr_etat_suivant)['etat_suivant' | 'proba' | 'recompense']\n",
        "#############################################################################\n",
        "def ConstructionTableTransitions(nbr_etats, nbr_actions, nbr_etat_suivants):\n",
        "  # Structure de la table de transition\n",
        "  structure_table = np.dtype([('etat_suivant', np.uint16),\n",
        "                            ('proba', np.float32),\n",
        "                            ('recompense', np.int8)])\n",
        "\n",
        "  # Initialisation de la table des transitions à 0\n",
        "  table_transition = np.zeros((nbr_etats, nbr_actions, nbr_etat_suivants),\n",
        "                              dtype=structure_table)\n",
        "\n",
        "  # Définition des états suivants\n",
        "  # A=0 ; B=1 et C=2\n",
        "  for etat in range(0,nbr_etats):\n",
        "    for action in range (0,nbr_actions):\n",
        "      table_transition[etat,action,0]['etat_suivant'] = 0\n",
        "      table_transition[etat,action,1]['etat_suivant'] = 1\n",
        "      table_transition[etat,action,2]['etat_suivant'] = 2\n",
        "  \n",
        "  # Définition des probabilités\n",
        "  # Etat A et B\n",
        "  for etat in range(0,2):\n",
        "    table_transition[etat,0,0]['proba'] = 0.2       # A->A (action=0)\n",
        "    table_transition[etat,0,1]['proba'] = 0.5       # A->B (action=0)\n",
        "    table_transition[etat,0,2]['proba'] = 0.3       # A->C (action=0)\n",
        "\n",
        "    table_transition[etat,1,0]['proba'] = 0.3       # A->A (action=1)\n",
        "    table_transition[etat,1,1]['proba'] = 0.2       # A->B (action=1)\n",
        "    table_transition[etat,1,2]['proba'] = 0.5       # A->C (action=1)\n",
        "\n",
        "  # Etat C\n",
        "  table_transition[2,0,0]['proba'] = 0.3            # C->A (action=0)\n",
        "  table_transition[2,0,1]['proba'] = 0.5            # C->B (action=0)\n",
        "  table_transition[2,0,2]['proba'] = 0.2            # C->C (action=0)\n",
        "\n",
        "  table_transition[2,1,0]['proba'] = 0.5            # C->A (action=1)\n",
        "  table_transition[2,1,1]['proba'] = 0.3            # C->B (action=1)\n",
        "  table_transition[2,1,2]['proba'] = 0.2            # C->C (action=1)\n",
        "\n",
        "  # Définition des récompenses\n",
        "  for etat in range(0,nbr_etats):\n",
        "    for action in range (0,nbr_actions):\n",
        "      for etat_suivant in range (0,nbr_etat_suivants):\n",
        "        # Si l'agent tombe sur un piège alors recompense = -1\n",
        "        if table_transition[etat,action,etat_suivant]['etat_suivant'] == 1:\n",
        "          table_transition[etat,action,etat_suivant]['recompense'] = -1\n",
        "\n",
        "        # Si l'agent tombe sur la cible alors recompense = 1 (sauf si piege)\n",
        "        elif table_transition[etat,action,etat_suivant]['etat_suivant'] == 2:\n",
        "          table_transition[etat,action,etat_suivant]['recompense'] = 1\n",
        "    \n",
        "  return table_transition"
      ],
      "metadata": {
        "id": "0NUol90yAZKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nombre_etats = 3\n",
        "nombre_actions = 2\n",
        "\n",
        "# Construction de la table des transitions\n",
        "table_transition = ConstructionTableTransitions(nombre_etats,nombre_actions,nombre_etats)\n",
        "\n",
        "# Converison de la table au format Pandas\n",
        "data = []\n",
        "columns = ['Etat', 'Action', 'Etat suivant', 'Proba', 'Récompense']\n",
        "for index_etat in range(0,nombre_etats):\n",
        "  for action in range(0,nombre_actions):\n",
        "    for etat_suivant in range(0,nombre_etats):\n",
        "      data.append([index_etat, action,\n",
        "                   table_transition[index_etat,action,etat_suivant]['etat_suivant'],\n",
        "                   table_transition[index_etat,action,etat_suivant]['proba'],\n",
        "                   table_transition[index_etat,action,etat_suivant]['recompense']])\n",
        "df_table = pd.DataFrame(data=data,columns=columns)\n",
        "\n",
        "# Affiche la table\n",
        "print(df_table.to_string(index=False))"
      ],
      "metadata": {
        "id": "ntk0a4XR-1tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Application de l'équation de Bellman**"
      ],
      "metadata": {
        "id": "8opdV8sUex40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commençons par appliquer l'équation de Bellman afin de calculer la valeur de l'action 0 lorsque l'agent se trouve sur l'état 0:"
      ],
      "metadata": {
        "id": "974PdT33jmGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Equation de Bellman\n",
        "# Q(s,a) = proba * (recompense + gamma * valeur_etat_suivant)\n",
        "\n",
        "gamma = 1\n",
        "\n",
        "# Initialisation de la table des valeurs d'état\n",
        "V = np.zeros((nombre_etats))         # (nombre_etats)\n",
        "\n",
        "# Initialisation de la table des valeurs d'action\n",
        "Q_table = np.zeros((nombre_etats,nombre_actions))         # (nombre_etats,nombre_actions)\n",
        "\n",
        "# Calcul de la valeur d'action de l'état A en suivant l'action 0\n",
        "# La valeur est calculé en sommant toutes les possibilités offertes\n",
        "# à l'agent qui suive le fait de prendre l'action 0 sur l'état A\n",
        "for etat_suivant in range(0,nombre_etats):\n",
        "  # Récupère la probabilité et la récompense\n",
        "  proba = table_transition[0,0,etat_suivant]['proba']\n",
        "  recompense = table_transition[0,0,etat_suivant]['recompense']\n",
        "\n",
        "  # Equation de Bellman\n",
        "  Q_table[0,0] = Q_table[0,0] + proba*(recompense + gamma*V[etat_suivant])\n",
        "\n",
        "print(Q_table[0,0])"
      ],
      "metadata": {
        "id": "EtopBZKae-oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut faire de même si c'est l'action 1 qui est prise sur l'état A:"
      ],
      "metadata": {
        "id": "HnVROyoXkj6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la valeur d'action de l'état A en suivant l'action 1\n",
        "for etat_suivant in range(0,nombre_etats):\n",
        "  # Récupère la probabilité et la récompense\n",
        "  proba = table_transition[0,1,etat_suivant]['proba']\n",
        "  recompense = table_transition[0,1,etat_suivant]['recompense']\n",
        "\n",
        "  # Equation de Bellman\n",
        "  Q_table[0,1] = Q_table[0,1] + proba*(recompense + gamma*V[etat_suivant])\n",
        "print(Q_table[0,1])"
      ],
      "metadata": {
        "id": "nSe8ej7MinNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons à quoi ressemble alors la table des valeurs d'actions :"
      ],
      "metadata": {
        "id": "g0b_JNuikpr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converision de la table des valeurs d'action au format Pandas\n",
        "data = []\n",
        "columns = ['Etat', 'Action', 'Valeur']\n",
        "for index_etat in range(0,nombre_etats):\n",
        "  for action in range(0,nombre_actions):\n",
        "    data.append([index_etat, action, Q_table[index_etat,action]])\n",
        "df_Qtable = pd.DataFrame(data=data,columns=columns)\n",
        "\n",
        "# Affiche la table\n",
        "print(\"Table des valeurs d'action:\\n\")\n",
        "print(df_Qtable.to_string(index=False))"
      ],
      "metadata": {
        "id": "xJWCWQCOi4uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durant l'apprentissage, l'agent suit la politique la plus optimisée possible, c'est-à-dire celle qui remporte le plus haut revenu. La règle pour l'agent est donc d'utiliser l'action avec la plus haute valeur. Dans ce cas, si l'agent se trouve sur l'état A, il suivra l'action 1.**"
      ],
      "metadata": {
        "id": "HXRmebIyk22E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La table des valeurs d'état est mise à jour en utilisant la plus haute valeur de la table des actions parmi toutes les actions possible sur l'état concerné:"
      ],
      "metadata": {
        "id": "VujQgDZ5lci_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V[0] = np.amax(Q_table[0,:])\n",
        "print(V)"
      ],
      "metadata": {
        "id": "BSewzodAlsVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après cette première itération, la table des états ressemble donc à celle-ci:"
      ],
      "metadata": {
        "id": "th3Ds9ZdmpPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<thead><tr>\n",
        "<th>État</th>\n",
        "<th>Valeur</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>A</td>\n",
        "<td>0.3</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>B</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>C</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "UmDrBwnIm7VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'agent suit la même procédure pour l'ensemble des états et répète le jeu:"
      ],
      "metadata": {
        "id": "KLHDZQXGmiuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Equation de Bellman\n",
        "# Q(s,a) = proba * (recompense + gamma * valeur_etat_suivant)\n",
        "\n",
        "gamma = 1\n",
        "\n",
        "# Initialisation de la table des valeurs d'état\n",
        "V = np.zeros((nombre_etats))         # (nombre_etats)\n",
        "\n",
        "# Initialisation de la table des valeurs d'action\n",
        "Q_table = np.zeros((nombre_etats,nombre_actions))         # (nombre_etats,nombre_actions)\n",
        "\n",
        "print(\"V::\",V)\n",
        "for i in range(5):\n",
        "  print(\"Épisode ... :\",i)\n",
        "  for etat in range(0,nombre_etats):\n",
        "    for action in range(0,nombre_actions):\n",
        "      for etat_suivant in range(0,nombre_etats):\n",
        "        # Récupère la probabilité et la récompense\n",
        "        proba = table_transition[etat,action,etat_suivant]['proba']\n",
        "        recompense = table_transition[etat,action,etat_suivant]['recompense']\n",
        "\n",
        "        # Equation de Bellman\n",
        "        Q_table[etat,action] = Q_table[etat,action] + proba*(recompense + gamma*V[etat_suivant])\n",
        "      \n",
        "    # Mise à jour de la table des valeurs d'états\n",
        "    V[etat] = np.amax(Q_table[etat,:])\n",
        "  print(\"V::\",V)"
      ],
      "metadata": {
        "id": "LXY2bmF8nIJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion de la table des valeurs d'action au format Pandas\n",
        "data = []\n",
        "columns = ['Etat', 'Action', 'Valeur']\n",
        "for index_etat in range(0,nombre_etats):\n",
        "  for action in range(0,nombre_actions):\n",
        "    data.append([index_etat, action, Q_table[index_etat,action]])\n",
        "df_Qtable = pd.DataFrame(data=data,columns=columns)\n",
        "\n",
        "# Affiche la table des valeurs d'actions\n",
        "print(\"Table des valeurs d'action:\\n\")\n",
        "print(df_Qtable.to_string(index=False))"
      ],
      "metadata": {
        "id": "1vPjyLoTof3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'agent, lorsqu'il part de l'état A (0) va donc prendre l'action 1 (haut-bas) et donc se diriger vers l'état C ce qui est logique car cela le dirige vers l'état C:"
      ],
      "metadata": {
        "id": "-8Hkj50bpW75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL16.png?raw=true\" width=\"240\"></img></center>"
      ],
      "metadata": {
        "id": "FAY_q4wmpV4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion de la table des valeurs d'état au format Pandas\n",
        "data = []\n",
        "columns = ['Etat', 'Valeur']\n",
        "for index_etat in range(0,nombre_etats):\n",
        "  data.append([index_etat, V[index_etat]])\n",
        "df_Vtable = pd.DataFrame(data=data,columns=columns)\n",
        "\n",
        "# Affiche la table des valeurs d'actions\n",
        "print(\"Table des valeurs d'état:\\n\")\n",
        "print(df_Vtable.to_string(index=False))"
      ],
      "metadata": {
        "id": "dYdd07MvoC0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il en de même pour les valeurs d'état : L'agent se dirige vers l'état C (2) cet état possède la plus haute valeur."
      ],
      "metadata": {
        "id": "GKYIZydKqH8E"
      }
    }
  ]
}