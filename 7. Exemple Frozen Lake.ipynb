{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGJ8yJ8W_3sA"
      },
      "source": [
        "#**Méthodes basées sur la programmation dynamique**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le cadre de l'apprentissage par renforcement, l'utilisation de méthodes de programmation dynamiques permet de calculer les solutions des fonctions de valeurs d'états et d'actions optimales. Cela nous permettra de déterminer par la suite la stratégie que l'agent doit suivre.\n",
        "\n",
        "Ces méthodes fonctionnent lorsque:\n",
        "- Le MDP possède un nombre fini et restreint d'états\n",
        "- Le modèle de l'environnement est parfaitement connu\n",
        "\n",
        "Nous allons étudier deux algorithmes qui sont:\n",
        "- L'algorithme par **itération des stratégies** (Policy Iteration). Cet algorithme comprend deux étapes principales:\n",
        "  - L'évaluation d'une stratégie (Policy Evaluation)\n",
        "  - L'amélioration d'une stratégie (Policy Improvement)\n",
        "\n",
        "- L'algorithme par **itération des valeurs** (Values Iteration)."
      ],
      "metadata": {
        "id": "LuuuM6cqnGbn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ1BfXx-VlXG"
      },
      "source": [
        "##**5. Algorithme par itération des stratégies**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4.5. Exemple**"
      ],
      "metadata": {
        "id": "0zqH5V_-pVTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reprenons l'exemple d'environnement proposé par OpenAI (Frozen Lake). Dans l'environnement Frozen Lake, l'agent peut se trouver sur 16 états et effectuer 4 actions par état : Haut, Bas, Gauche, Droite.\n",
        "\n",
        "&nbsp;\n",
        "<br/>\n",
        "&nbsp;\n",
        "<br/>\n",
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL10.png?raw=true\" width=\"640\"></center>\n",
        "&nbsp;\n",
        "<br/>\n",
        "&nbsp;\n",
        "<br/>"
      ],
      "metadata": {
        "id": "ud_AVjbHp5RJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cet environnement, il y a donc 4 trous (états 5, 7, 11 et 12) et un objectif (état 15). L'agent part de l'état 0 et son objectif est d'atteindre l'état 15 en trouvant le chemin le plus court. On assume que l'agent reçoit une récompense de +1 si il atteint l'objectif, de 0 s'il tombe dans un trou (et le jeu se termine) et dans les autres cas.\n",
        "\n",
        "On considère qu'il y a 1/3 de chance que l'agent effectue correctement l'action choisie. Il y a donc 2/3 de chance que l'action soit défaillante. Mais attention, les autres actions possibles dans la liste des actions défaillantes sont celles qui sont les \"plus proches\" de l'action originale. Par exemple:\n",
        "\n",
        "- Action choisie: \"Droite\":\n",
        "  - 33% de chance d'aller à droite\n",
        "  - 33% de chance d'aller en haut\n",
        "  - 33% de chance d'aller en bas\n",
        "\n",
        "- Action chosie \"Haut\":\n",
        "  - 33% de chance d'aller en haut\n",
        "  - 33% de chance d'aller à gauche\n",
        "  - 33% de chance d'aller à droite   \n",
        "\n",
        "...\n",
        "\n",
        "Le programme suivant définit l'environnement Frozen Lake et affiche un apperçu de la table des transitions."
      ],
      "metadata": {
        "id": "z1fAO36Ap8-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Chargement de l'environnement\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Remise à zéro de l'environnement et affichage de la carte\n",
        "env.reset()                    \n",
        "\n",
        "# Récupération de la table des transitions\n",
        "P = env.env.P\n",
        "\n",
        "# Affichage des valeurs contenues dans la table\n",
        "for key, value in P.items():\n",
        "    print (key, \": \", value)"
      ],
      "metadata": {
        "id": "d3Blqx-qpfiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithme d'évaluation d'une stratégie**\n",
        "\n",
        "On définit maintenant l'algorithme permettant d'évaluer une stratégie (partie 2 de la figure ci-dessous) :"
      ],
      "metadata": {
        "id": "HWucyBY7q1ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL30.png?raw=true\" width=\"640\"></center>"
      ],
      "metadata": {
        "id": "5RCgSnoArCmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction permettant d'évaluer une stratégie\n",
        "# pi :  pi(a|s)\n",
        "# P :   Table des transitions\n",
        "# V_ :  Fonction des valeurs d'états issue de l'amélioration de la stratégie précédente\n",
        "# gamma : Facteur de remise\n",
        "# theta : Seuil déterminant la précision de l'évaluation\n",
        "\n",
        "def evaluation_strategie(pi, P, V_, gamma=0.99, theta=1e-10):\n",
        "    # Répéter tant que l'algorithme n'a pas convergé\n",
        "    delta = 2*theta\n",
        "    while delta > theta:\n",
        "        # Initialise la fonction valeur d'état à 0\n",
        "        V = np.zeros(len(P), dtype=np.float64)\n",
        "\n",
        "        # Pour chaque état de l'environnement\n",
        "        for s in range(len(P)):\n",
        "            # Calcule la nouvelle valeur de l'état en cours à l'aide de la\n",
        "            # fonction des valeurs d'états issus de l'amélioration de la\n",
        "            # stratégie précédente\n",
        "            for proba, etat_suivant, recompense, etat_terminal in P[s][pi(s)]:\n",
        "                  V[s] += proba*(recompense + gamma*V_[etat_suivant])\n",
        "            \n",
        "        # Compare la fonction des valeurs des états obtenue avec la précédente\n",
        "        delta = np.max(np.abs(V_ - V))\n",
        "        print(delta)\n",
        "\n",
        "        # Sauvegarde la fonction des valeurs des états pour la prochaine itération\n",
        "        V_ = V.copy()\n",
        "    return V"
      ],
      "metadata": {
        "id": "cr3bhUiFrF-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithme d'amélioration d'une stratégie**\n",
        "\n",
        "On définit maintenant l'algorithme permettant l'amélioration d'une stratégie (partie 3 de la figure ci-dessous) :"
      ],
      "metadata": {
        "id": "uhtD7Wdkshar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL30.png?raw=true\" width=\"640\"></center>"
      ],
      "metadata": {
        "id": "hlfPpgEashat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction permettant d'améliorer une stratégie\n",
        "#  pi :     Stratégie à améliorer\n",
        "#  V :      Fonction de valeurs des états\n",
        "#  P :      Table de transition\n",
        "#  gamma :  Facteur de remise\n",
        "\n",
        "def amelioration_strategie(pi, V, P, gamma=0.99):\n",
        "    # Initalise la fonction des valeurs d'actions\n",
        "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
        "\n",
        "    # Initialise l'indicateur de strategie stabilisée\n",
        "    strategie_stable = True\n",
        "\n",
        "    # Pour chaque état de l'environnement\n",
        "    for s in range(len(P)):\n",
        "        # Pour chaque action disponnible sur cet état\n",
        "        for a in range(len(P[s])):\n",
        "            # Calcul de la valeur d'action Q(s,a)\n",
        "            for proba, etat_suivant, recompense, etat_terminal in P[s][a]:\n",
        "                Q[s][a] += proba*(recompense + gamma*V[etat_suivant])\n",
        "            \n",
        "        # Récupère l'action la plus forte sur cet état\n",
        "        a_max = np.argmax(Q[s,:])\n",
        "\n",
        "        # Compare cette action avec celle donnée par la stratégie à optimiser\n",
        "        if a_max != pi(s):\n",
        "          strategie_stable = False\n",
        "\n",
        "    #compute new policy which is going to be compared. The iteration will brake if difference is under the threshold\n",
        "    nouvelle_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "    return nouvelle_pi, strategie_stable"
      ],
      "metadata": {
        "id": "NlL5LJVFr-u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithme général d'itération des stratégies**\n",
        "\n",
        "On définit maintenant l'algorithme général permettant de trouver une stratégie optimale par la méthode d'itération des stratégies:"
      ],
      "metadata": {
        "id": "WF5s4VpKsyBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/RL/Concept_RL30.png?raw=true\" width=\"640\"></center>"
      ],
      "metadata": {
        "id": "uqCUA1IVsyBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Algorithme d'itération des stratégies \n",
        "gamma = 0.99    # Facteur de remise\n",
        "theta = 1e-10   # Seuil de précision de l'évaluation des stratégies\n",
        "\n",
        "# Initialise une stratégie aléatoire pi(s)\n",
        "actions_aleatoires = np.random.choice([0,1,2,3],len(P))\n",
        "pi = lambda s: {s:a for s, a in enumerate(actions_aleatoires)}[s]               # pi(s) = action de la stratégie pi\n",
        "\n",
        "strategie_stable = False\n",
        "\n",
        "# Initialisation de la fonction des valeurs d'états à 0\n",
        "V_ = np.zeros(len(P), dtype=np.float64)\n",
        "\n",
        "while strategie_stable is False:\n",
        "  print (\"Stratégie : %s\" %{s:pi(s) for s in range(len(P))})\n",
        "  # Évaluation de la stratégie\n",
        "  V = evaluation_strategie(pi, P, V_, gamma, theta)\n",
        "\n",
        "  # Amélioration de la stratégie\n",
        "  pi, strategie_stable = amelioration_strategie(pi, V, P, gamma)"
      ],
      "metadata": {
        "id": "39VjqY_9tT0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affichage de la stratégie et de la fonction des valeurs des états**"
      ],
      "metadata": {
        "id": "14hnhCTW7QG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def affiche_strategie(pi, P, objectif, symboles_actions=('<', 'v', '>', '^'), n_cols=4, titre='Stratégie optimale:'):\n",
        "    print(titre)\n",
        "    arrs = {k:v for k,v in enumerate(symboles_actions)}\n",
        "    for s in range(len(P)):\n",
        "        a = pi(s)\n",
        "        print(\"| \", end=\"\")\n",
        "        if s == objectif:\n",
        "            print(\"OBJ\".rjust(6), end=\" \")\n",
        "        \n",
        "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
        "            if s != objectif:\n",
        "                print(\"TROU\".rjust(6), end=\" \")\n",
        "        else:\n",
        "            #print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
        "            print(arrs[a].rjust(6), end=\" \")\n",
        "        if (s + 1) % n_cols == 0: print(\"|\")"
      ],
      "metadata": {
        "id": "HUGAarQg7JbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def affiche_fonction_valeurs_actions(V, P, objectif, n_cols=4, prec=3, titre=\"Fonction des valeurs d'états:\"):\n",
        "    print(titre)\n",
        "    for s in range(len(P)):\n",
        "        v = V[s]\n",
        "        print(\"| \", end=\"\")\n",
        "        \n",
        "        if s == objectif:\n",
        "            print(\"OBJ\".rjust(6), end=\" \")\n",
        "        \n",
        "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
        "            if s != objectif:\n",
        "                print(\"TROU\".rjust(6), end=\" \")\n",
        "        else:\n",
        "            print('{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
        "        if (s + 1) % n_cols == 0: print(\"|\")"
      ],
      "metadata": {
        "id": "EnJJ5AZtCaqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "affiche_strategie(pi,P,15)\n",
        "print(\"\")\n",
        "affiche_fonction_valeurs_actions(V, P, 15)"
      ],
      "metadata": {
        "id": "XjiZMeK-7qSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4.6. Exemple avec des probabilités d'actions quasi-déterministes**"
      ],
      "metadata": {
        "id": "jEY2jmkEEz8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Chargement de l'environnement\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Remise à zéro de l'environnement et affichage de la carte\n",
        "env.reset()                    \n",
        "\n",
        "# Récupération de la table des transitions\n",
        "P = env.env.P"
      ],
      "metadata": {
        "id": "DlD8LoYdFmTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modification des récompenses\n",
        "recompense_objectif, recompense_trou, recopense_autre = 1, -1, -0.04\n",
        "objectif, trou = 15, [5, 7, 11, 12]\n",
        "for s in range(len(P)):\n",
        "    for a in range(len(P[s])):\n",
        "        for t in range(len(P[s][a])):\n",
        "            values = list(P[s][a][t])\n",
        "            if values[1] == objectif:\n",
        "                values[2] = recompense_objectif\n",
        "                values[3] = False\n",
        "            elif values[1] in trou:\n",
        "                values[2] = recompense_trou\n",
        "                values[3] = False\n",
        "            else:\n",
        "                values[2] = recopense_autre\n",
        "                values[3] = False\n",
        "            if s in trou or s == objectif:\n",
        "                values[2] = 0\n",
        "                values[3] = True\n",
        "            P[s][a][t] = tuple(values)\n",
        "\n",
        "# Modification de la table des transitions\n",
        "prob_action, prob_glisse_1, prob_glisse_2 = 0.8, 0.1, 0.1\n",
        "for s in range(len(P)):\n",
        "    for a in range(len(P[s])):\n",
        "        for t in range(len(P[s][a])):\n",
        "            if P[s][a][t][0] == 1.0:\n",
        "                continue\n",
        "            values = list(P[s][a][t])\n",
        "            if t == 0:\n",
        "                values[0] = prob_glisse_1\n",
        "            elif t == 1:\n",
        "                values[0] = prob_action\n",
        "            elif t == 2:\n",
        "                values[0] = prob_glisse_2\n",
        "            P[s][a][t] = tuple(values)"
      ],
      "metadata": {
        "id": "rVFdYB8_EzQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des valeurs contenues dans la table\n",
        "for key, value in P.items():\n",
        "    print (key, \": \", value)"
      ],
      "metadata": {
        "id": "XNZBxc7RFshV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algorithme d'itération des stratégies \n",
        "gamma = 0.99    # Facteur de remise\n",
        "theta = 1e-10   # Seuil de précision de l'évaluation des stratégies\n",
        "\n",
        "# Initialise une stratégie aléatoire pi(s)\n",
        "actions_aleatoires = np.random.choice([0,1,2,3],len(P))\n",
        "pi = lambda s: {s:a for s, a in enumerate(actions_aleatoires)}[s]               # pi(s) = action de la stratégie pi\n",
        "\n",
        "strategie_stable = False\n",
        "\n",
        "# Initialisation de la fonction des valeurs d'états à 0\n",
        "V_ = np.zeros(len(P), dtype=np.float64)\n",
        "\n",
        "while strategie_stable is False:\n",
        "  print (\"Stratégie : %s\" %{s:pi(s) for s in range(len(P))})\n",
        "  # Évaluation de la stratégie\n",
        "  V = evaluation_strategie(pi, P, V_, gamma, theta)\n",
        "\n",
        "  # Amélioration de la stratégie\n",
        "  pi, strategie_stable = amelioration_strategie(pi, V, P, gamma)"
      ],
      "metadata": {
        "id": "H_dJPAyPFw3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "affiche_strategie(pi,P,15)\n",
        "print(\"\")\n",
        "affiche_fonction_valeurs_actions(V, P, 15)"
      ],
      "metadata": {
        "id": "PbvwD9eDF1R9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "7. Exemple d'itération des stratégies.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}